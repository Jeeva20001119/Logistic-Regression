{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**1. What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?**\n",
        "\n",
        "* 1. Logistic Regression\n",
        "  * Logistic Regression is a statistical/machine learning algorithm used for classification tasks (predicting categorical outcomes).\n",
        "  *  Instead of predicting continuous values, it predicts the probability that a given input belongs to a certain class (usually binary: 0 or 1).\n",
        "  * Mathematics:Linear combination of inputs is computed first:\n",
        "        z=w0+w1x1+w2x2+⋯+wnxn\n",
        "\t* Then passed through the sigmoid (logistic) function: p=1+e−z1\n",
        "\t* ​Output p lies between 0 and 1, representing probability.\n",
        "  * A threshold (e.g., 0.5) is applied to classify into categories.\n",
        "* 2. Linear Regression\n",
        "   * Linear Regression is used for regression tasks (predicting continuous outcomes).\n",
        "  *  Directly predicts a numeric value, not probabilities.\n",
        "  * Mathematics:\n",
        "      * y=w0​+w1​x1​+w2​x2​+⋯+wn​xn​\n",
        "\t* ​Here y can take any real value (−∞ to +∞).\n",
        "\n",
        "* **Key Differences Linear Regression**\n",
        "\n",
        "    * Aspect\t ->Linear Regression\n",
        "    * Type of Problem -> Regression (predict continuous values)\n",
        "    * Output Range -> −∞ to +∞ (real numbers)\n",
        "    * Algorithm Goal  -> Minimize squared error (MSE)\n",
        "    * Prediction  -> Continuous value (e.g., house price)\n",
        "    * Function Used -> Linear function\n",
        "\n",
        "* **Key Differences Logistic Regression**\n",
        "\n",
        "   * Aspect\t ->Logistic Regression\n",
        "  * Type of Problem -> Classification (predict categories, usually binary)\n",
        "  * Output Range -> 0 to 1 (probabilities via sigmoid)\n",
        "  * Algorithm Goal  -> Maximize likelihood (via log-loss / cross-entropy)\n",
        "  * Prediction  -> Probability/class label (e.g., spam vs. not spam)\n",
        "  * Function Used -> Logistic (sigmoid) function\n",
        "\n",
        "**2. Explain the role of the Sigmoid function in Logistic Regression?**\n",
        "\n",
        "* 1. Transforms Linear Output into Probability\n",
        "  * Logistic Regression first computes a linear combination of inputs:\n",
        "  * z=w0​+w1​x1​+w2​x2​+⋯+wn​xn​\n",
        "  * This z can take any real value (from −∞ to +∞).\n",
        "  * But probabilities must always lie in the range [0, 1]\n",
        "  * The sigmoid function fixes this:σ(z)=1+e−z1​\n",
        "  * It \"squashes\" any real number into a value between 0 and 1.\n",
        "* 2. Interpreting Output as Probability\n",
        "  * After applying sigmoid, the output represents:P(y=1∣X)=σ(z)\n",
        "  * Example: If sigmoid output is 0.85 → there is an 85% probability that the input belongs to class 1.\n",
        "* 3. Decision Boundary\n",
        "  * Logistic Regression uses a threshold (commonly 0.5) on the sigmoid output to assign classes:\n",
        "     * If σ(z)≥0.5⇒ predict class 1\n",
        "     * If σ(z)< 0.5⇒ predict class 0.\n",
        "* 4. Smooth Gradient for Optimization\n",
        "  * The sigmoid function is differentiable, which is essential for optimization using Gradient Descent.\n",
        "  * Its derivative:σ′(z)=σ(z)(1−σ(z))\n",
        "  * This property makes weight updates efficient during training.\n",
        "\n",
        "\n",
        "**3. What is Regularization in Logistic Regression and why is it needed?**\n",
        "\n",
        "* Regularization is a technique used to prevent overfitting in machine learning models.\n",
        "* It works by adding a penalty term to the model’s loss function, discouraging the model from assigning too large weights to the features.\n",
        "* In Logistic Regression, the modified cost function looks like: J(w)=−m1​i=1∑m​[y(i)log(y^​(i))+(1−y(i))log(1−y^​(i))]+λ⋅Penalty(w)\n",
        "* Here: m = number of samples\n",
        "  * y^(i)= predicted probability\n",
        "  * λ = regularization strength (controls penalty size)\n",
        "  * Penalty(w) = term based on weights w\n",
        "* Types of Regularization\n",
        "* 1. L2 Regularization (Ridge)\n",
        "   * Penalty: Penalty(w)= j∑wj2\n",
        "\t​ * Encourages small but nonzero weights.\n",
        "   * Prevents model from relying too much on a single feature.\n",
        "* 2. L1 Regularization (Lasso)\n",
        "   * Penalty: Penalty(w)=j∑ ∣wj∣\n",
        "   * Encourages sparsity → pushes some weights to exactly zero.\n",
        "   * Useful for feature selection.\n",
        "* 3. Elastic Net\n",
        "   * Combination of L1 and L2 regularization.\n",
        "\n",
        "* Why is Regularization Needed?\n",
        "* Logistic Regression can overfit when:\n",
        "     * There are too many features.\n",
        "     * Features are highly correlated (multicollinearity).\n",
        "     * Training data is small or noisy.\n",
        "* Overfitting means the model learns noise instead of general patterns → performs poorly on unseen data.\n",
        "\n",
        "* Regularization helps by:\n",
        "   * Controlling model complexity.\n",
        "   * Preventing large weights.\n",
        "   * Improving generalization to new data.\n",
        "\n",
        "* 4. Intuition Example\n",
        "* Suppose you’re predicting if an email is spam.\n",
        "* Without regularization, the model might assign an extremely high weight to a rare word (e.g., \"lottery\"), making predictions unstable.\n",
        "* With regularization, the model balances weights across features, improving stability and accuracy.\n",
        "\n",
        "**4. What are some common evaluation metrics for classification models, and\n",
        "why are they important?**\n",
        "\n",
        "* When we build a classification model (like Logistic Regression, Decision Trees, etc.), we need to measure how well it performs. Different metrics highlight different aspects of performance.\n",
        "* 1. Accuracy\n",
        "   * Accuracy=Correct Predictions/Total Predictions\n",
        "   * Example: If 90 out of 100 predictions are correct → Accuracy = 90%.\n",
        "  * Usefulness: Simple and intuitive.\n",
        "  * Limitation: Misleading when classes are imbalanced (e.g., 95% \"not spam\" and 5% \"spam\" → model predicting always \"not spam\" gives 95% accuracy but is useless).\n",
        "* 2. Precision\n",
        "   *  Among the samples predicted as positive, how many are actually positive?\n",
        "   * Precision=TP+FP/TP\n",
        "\t​ * Usefulness: Important when false positives are costly (e.g., predicting \"cancer\" when it’s not).\n",
        "* 3. Recall (Sensitivity or True Positive Rate)\n",
        "  *  Among the actual positives, how many did the model correctly identify?\n",
        "  * Recall=TP+FN/TP\n",
        "\t​* Usefulness: Important when false negatives are costly (e.g., missing an actual cancer patient).\n",
        "* 4. F1-Score\n",
        "  *  Harmonic mean of Precision and Recall.\n",
        "  * F1=2⋅Precision+Recall/Precision⋅Recall\n",
        "\t​* Usefulness: Balances Precision and Recall. Useful when dataset is imbalanced.\n",
        "* 5. ROC Curve & AUC (Area Under Curve)\n",
        "  * ROC Curve: Plots True Positive Rate (Recall) vs False Positive Rate at different thresholds.\n",
        "  * AUC: Measures area under ROC curve (closer to 1 is better).\n",
        "  * Usefulness: Evaluates model performance across thresholds instead of a single cutoff (like 0.5).\n",
        "* 6. Confusion Matrix\n",
        "   * A table showing:\n",
        "      * TP (True Positives)\n",
        "      * TN (True Negatives)\n",
        "      * FP (False Positives)\n",
        "      * FN (False Negatives)\n",
        "* Usefulness: Gives a complete picture of errors.\n",
        "* Different problems have different priorities:\n",
        "   * Medical diagnosis → Recall is more important (don’t miss actual cases).\n",
        "   * Spam filter → Precision is more important (don’t classify normal mail as spam).\n",
        "   * Accuracy alone can be misleading, especially with imbalanced datasets.\n",
        "   * Using multiple metrics helps understand trade-offs between errors.\n",
        "\n",
        "**5.  Write a Python program that loads a CSV file into a Pandas DataFrame,\n",
        "splits into train/test sets, trains a Logistic Regression model, and prints its accuracy. (Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)**\n"
      ],
      "metadata": {
        "id": "Yp6oSVLKWpAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# 1. Load dataset from sklearn\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Show first 5 rows\n",
        "print(\"First 5 rows of dataset:\")\n",
        "print(df.head(), \"\\n\")\n",
        "\n",
        "# 2. Split into features (X) and target (y)\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "# 3. Split into Train/Test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 4. Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=5000)  # increase iterations for convergence\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 6. Calculate Accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of Logistic Regression model: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpDl9-EUof4Q",
        "outputId": "535c0b2b-6bda-4936-a5a9-7447eb6e440c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 rows of dataset:\n",
            "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
            "0        17.99         10.38          122.80     1001.0          0.11840   \n",
            "1        20.57         17.77          132.90     1326.0          0.08474   \n",
            "2        19.69         21.25          130.00     1203.0          0.10960   \n",
            "3        11.42         20.38           77.58      386.1          0.14250   \n",
            "4        20.29         14.34          135.10     1297.0          0.10030   \n",
            "\n",
            "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
            "0           0.27760          0.3001              0.14710         0.2419   \n",
            "1           0.07864          0.0869              0.07017         0.1812   \n",
            "2           0.15990          0.1974              0.12790         0.2069   \n",
            "3           0.28390          0.2414              0.10520         0.2597   \n",
            "4           0.13280          0.1980              0.10430         0.1809   \n",
            "\n",
            "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
            "0                 0.07871  ...          17.33           184.60      2019.0   \n",
            "1                 0.05667  ...          23.41           158.80      1956.0   \n",
            "2                 0.05999  ...          25.53           152.50      1709.0   \n",
            "3                 0.09744  ...          26.50            98.87       567.7   \n",
            "4                 0.05883  ...          16.67           152.20      1575.0   \n",
            "\n",
            "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
            "0            0.1622             0.6656           0.7119                0.2654   \n",
            "1            0.1238             0.1866           0.2416                0.1860   \n",
            "2            0.1444             0.4245           0.4504                0.2430   \n",
            "3            0.2098             0.8663           0.6869                0.2575   \n",
            "4            0.1374             0.2050           0.4000                0.1625   \n",
            "\n",
            "   worst symmetry  worst fractal dimension  target  \n",
            "0          0.4601                  0.11890       0  \n",
            "1          0.2750                  0.08902       0  \n",
            "2          0.3613                  0.08758       0  \n",
            "3          0.6638                  0.17300       0  \n",
            "4          0.2364                  0.07678       0  \n",
            "\n",
            "[5 rows x 31 columns] \n",
            "\n",
            "Accuracy of Logistic Regression model: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Write a Python program to train a Logistic Regression model using L2\n",
        "regularization (Ridge) and print the model coefficients and accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)**"
      ],
      "metadata": {
        "id": "bRmXI21Goojs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df['target'] = data.target\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "# 2. Split dataset into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Logistic Regression with L2 Regularization (Ridge)\n",
        "# C controls regularization strength (smaller C = stronger regularization)\n",
        "model = LogisticRegression(penalty='l2', C=1.0, max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predictions and Accuracy\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# 5. Print Model Coefficients and Accuracy\n",
        "print(\"Model Coefficients (first 10 shown):\")\n",
        "print(model.coef_[0][:10])  # printing only first 10 for readability\n",
        "print(\"\\nIntercept:\", model.intercept_[0])\n",
        "print(f\"\\nAccuracy of Logistic Regression with L2 Regularization: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw5oow-ko-sE",
        "outputId": "e9909795-e536-4019-e058-64d493c24074"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Coefficients (first 10 shown):\n",
            "[ 1.0274368   0.22145051 -0.36213488  0.0254667  -0.15623532 -0.23771256\n",
            " -0.53255786 -0.28369224 -0.22668189 -0.03649446]\n",
            "\n",
            "Intercept: 28.648713947072245\n",
            "\n",
            "Accuracy of Logistic Regression with L2 Regularization: 0.9561\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7.Write a Python program to train a Logistic Regression model for multiclass\n",
        "classification using multi_class='ovr' and print the classification report.\n",
        "(Use Dataset from sklearn package)**"
      ],
      "metadata": {
        "id": "YPVlssDapDNh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# 1. Load dataset\n",
        "iris = load_iris()\n",
        "\n",
        "# Convert to Pandas DataFrame\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "df['target'] = iris.target\n",
        "\n",
        "# Features and target\n",
        "X = df.drop(\"target\", axis=1)\n",
        "y = df[\"target\"]\n",
        "\n",
        "# 2. Split dataset into train/test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Train Logistic Regression with One-vs-Rest (OvR)\n",
        "model = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=5000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# 5. Print Classification Report\n",
        "print(\"Classification Report for Logistic Regression (OvR):\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zxpMUxVpS5E",
        "outputId": "e55c860b-063c-4f15-aee2-b7c23cd4c641"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report for Logistic Regression (OvR):\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       1.00      0.89      0.94         9\n",
            "   virginica       0.92      1.00      0.96        11\n",
            "\n",
            "    accuracy                           0.97        30\n",
            "   macro avg       0.97      0.96      0.97        30\n",
            "weighted avg       0.97      0.97      0.97        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Write a Python program to apply GridSearchCV to tune C and penalty\n",
        "hyperparameters for Logistic Regression and print the best parameters and validation\n",
        "accuracy.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)**"
      ],
      "metadata": {
        "id": "uB6Gl9pOpcJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# 1. Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# 2. Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 3. Define Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=5000, solver='liblinear')\n",
        "# (liblinear supports both l1 and l2 penalties)\n",
        "\n",
        "# 4. Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],   # Regularization strength\n",
        "    'penalty': ['l1', 'l2']         # L1 = Lasso, L2 = Ridge\n",
        "}\n",
        "\n",
        "# 5. Apply GridSearchCV\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6. Print best parameters and validation accuracy\n",
        "print(\"Best Parameters found:\", grid_search.best_params_)\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tpClpv9psfx",
        "outputId": "4901215c-8b43-44f9-c781-b12fce9cdedf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters found: {'C': 10, 'penalty': 'l1'}\n",
            "Best Cross-Validation Accuracy: 0.9583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling.\n",
        "(Use Dataset from sklearn package)\n",
        "(Include your Python code and output in the code box below.)**"
      ],
      "metadata": {
        "id": "xyfN7fWdpwmg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jdnX2uYHtX_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "# 1. Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# 2. Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# -------- Model WITHOUT Scaling --------\n",
        "model_no_scaling = LogisticRegression(max_iter=5000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# -------- Model WITH Scaling --------\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=5000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# 3. Print Results\n",
        "print(\"Accuracy WITHOUT Scaling: {:.4f}\".format(accuracy_no_scaling))\n",
        "print(\"Accuracy WITH Scaling   : {:.4f}\".format(accuracy_scaled))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBRWumlhqATG",
        "outputId": "7bb98ca8-d907-4278-d554-17d6b8f162d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT Scaling: 0.9561\n",
            "Accuracy WITH Scaling   : 0.9737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced dataset (only 5% of customers respond), describe the approach you’d take to build a Logistic Regression model — including data handling, feature scaling, balancing classes, hyperparameter tuning, and evaluating the model for this real-world business use case.**"
      ],
      "metadata": {
        "id": "qiNpV3PnqFFl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Approach to Building the Model**\n",
        "\n",
        "* 1. Data Understanding & Cleaning\n",
        "  * Check missing values → impute or drop as appropriate.\n",
        "  * Handle outliers (e.g., extremely high purchase amounts).\n",
        "  * Feature engineering:\n",
        "      * Recency, frequency, and monetary value (RFM features).\n",
        "      * Past response history.\n",
        "  * Demographics, engagement data (website visits, email opens).\n",
        "* 2. Feature Scaling\n",
        "  * Logistic Regression is sensitive to feature scale.\n",
        "  * Apply StandardScaler or MinMaxScaler to ensure features like age (years) and income ($) are on similar scales.\n",
        "  * Scaling helps coefficients be more meaningful and improves convergence.\n",
        "* 3. Handling Class Imbalance (only 5% responders)\n",
        "   * Imbalance is critical here — otherwise, the model will predict “no response” for everyone and still achieve ~95% accuracy.\n",
        "   * Options:\n",
        "   * 1.Resampling Techniques\n",
        "        * Oversampling minority class (e.g., SMOTE – Synthetic Minority Oversampling Technique).\n",
        "        * Undersampling majority class (downsample non-responders).\n",
        "        * Sometimes a hybrid works best.\n",
        "\n",
        "    * 2.Class Weights\n",
        "        * In scikit-learn:\n",
        "        * LogisticRegression(class_weight='balanced')\n",
        "        * Penalizes misclassification of minority class more heavily.\n",
        "* 4. Model Training & Hyperparameter Tuning\n",
        "    * Logistic Regression hyperparameters:\n",
        "    \n",
        "            * C (inverse regularization strength).\n",
        "            * Penalty (l1, l2, elasticnet).\n",
        "            * Solver (must match penalty type).\n",
        "    * Use GridSearchCV or RandomizedSearchCV with cross-validation.\n",
        "* Example grid: param_grid = {\n",
        "    \"C\": [0.01, 0.1, 1, 10],\n",
        "    \"penalty\": [\"l1\", \"l2\"],\n",
        "    \"solver\": [\"liblinear\", \"saga\"]\n",
        "}\n",
        "* 5. Evaluation Metrics\n",
        "   * Accuracy is not useful in imbalanced datasets → instead:\n",
        "   * Precision: Of predicted responders, how many actually responded?\n",
        "   * Recall (Sensitivity): Of actual responders, how many did we capture?\n",
        "   * F1-score: Balance between Precision and Recall.\n",
        "   * ROC-AUC: Probability that the model ranks a random responder higher than a non-responder.\n",
        "   * PR-AUC (Precision-Recall AUC): Especially valuable in highly imbalanced data.\n",
        "   * If campaign cost is high → prioritize Precision (target fewer, but more accurate).\n",
        "   * If missing a potential customer is worse → prioritize Recall.\n",
        "   * Best: tune decision threshold to maximize expected profit.\n",
        "* 6. Business Deployment\n",
        "   * Use the model’s predicted probability, not just hard 0/1.\n",
        "   * Rank customers by probability → run campaigns on top N% most likely responders.\n",
        "   * Continuously retrain as new campaign results arrive."
      ],
      "metadata": {
        "id": "S_JxN28EqiNi"
      }
    }
  ]
}